import{_ as r,r as i,o as p,c,a,b as e,d as s,w as o,e as t}from"./app-7ac5536a.js";const d={},u=a("h1",{id:"installing-fedml",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#installing-fedml","aria-hidden":"true"},"#"),e(" Installing FedML")],-1),m=a("p",null,"FedML supports Linux, MacOS, Windows, and Android.",-1),h=a("h2",{id:"fedml-source-code-repository",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#fedml-source-code-repository","aria-hidden":"true"},"#"),e(" FedML Source Code Repository")],-1),v={href:"https://github.com/FedML-AI/FedML",target:"_blank",rel:"noopener noreferrer"},b=t(`<h2 id="installing-with-pip" tabindex="-1"><a class="header-anchor" href="#installing-with-pip" aria-hidden="true">#</a> Installing with pip</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip <span class="token function">install</span> fedml
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>if your machine has not installed python, please install one version of the following pythons: 3.8, 3.9, 3.10.</p><h3 id="installing-with-pip-on-ubuntu" tabindex="-1"><a class="header-anchor" href="#installing-with-pip-on-ubuntu" aria-hidden="true">#</a> Installing with pip on Ubuntu</h3><p>On Ubuntu, run the following commands to install pip3 and fedml.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> python3-pip
pip3 <span class="token function">install</span> fedml
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$HOME</span>/.local/bin:<span class="token environment constant">$PATH</span>
fedml <span class="token function">env</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="installing-with-pip-on-centos" tabindex="-1"><a class="header-anchor" href="#installing-with-pip-on-centos" aria-hidden="true">#</a> Installing with pip on CentOS</h3><p>On CentOS, run the following commands to install pip3 and fedml.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>yum –y <span class="token function">install</span> python3-pip
pip3 <span class="token function">install</span> fedml
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$HOME</span>/.local/bin:<span class="token environment constant">$PATH</span>
fedml <span class="token function">env</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The default machine learning engine is <code>PyTorch</code>. FedML also supports <code>TensorFlow</code>, <code>Jax</code>, and <code>MXNet</code>. You can install related engines as follows:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip <span class="token function">install</span> <span class="token string">&quot;fedml[MPI]&quot;</span>
pip <span class="token function">install</span> <span class="token string">&quot;fedml[tensorflow]&quot;</span>
pip <span class="token function">install</span> <span class="token string">&quot;fedml[jax]&quot;</span>
pip <span class="token function">install</span> <span class="token string">&quot;fedml[mxnet]&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>For MPI installation, it&#39;s used for local distributed simulation with MPI (https://mpi4py.readthedocs.io/en/stable/). On MacOS, the installation commands in conda environment is:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>conda <span class="token function">install</span> mpi4py openmpi
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>`,13),k={href:"https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html",target:"_blank",rel:"noopener noreferrer"},g={href:"https://formulae.brew.sh/formula/open-mpi",target:"_blank",rel:"noopener noreferrer"},f={href:"https://formulae.brew.sh/formula/open-mpi",target:"_blank",rel:"noopener noreferrer"},y=t(`<p>The above commands work properly in Linux environment. For Windows/Mac OS (Intel)/Mac OS (M1), you may need to follow TensorFlow/Jax/MXNet official guidance to fix related installation issues.</p><h2 id="installing-fedml-with-anaconda" tabindex="-1"><a class="header-anchor" href="#installing-fedml-with-anaconda" aria-hidden="true">#</a> Installing FedML with Anaconda</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>conda create <span class="token parameter variable">--name</span> fedml-pip <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span>
conda activate fedml-pip
conda <span class="token function">install</span> <span class="token parameter variable">--name</span> fedml-pip pip
pip <span class="token function">install</span> fedml
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Note</strong>: please use python 3.8 if you met any compatability issues. Currently, we support 3.7, 3.8, 3.9, 3.10, 3.11.</p><p>On MacOS and Python 3.11, if you meet any issues, please run the command:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>brew <span class="token function">install</span> autoconf automake libffi libtool pkg-config
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>Then run the command to install fedml again:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip <span class="token function">install</span> fedml
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>After installation, please use &quot;pip list | grep fedml&quot; to check whether <code>fedml</code> is installed.</p><h2 id="compatibility-with-homebrew-installed-python-on-apple-silicon-mac" tabindex="-1"><a class="header-anchor" href="#compatibility-with-homebrew-installed-python-on-apple-silicon-mac" aria-hidden="true">#</a> Compatibility with HomeBrew-Installed Python On Apple Silicon Mac</h2><p>If you are using Apple Silicon MAC, we suggest using Conda to install python 3.8+ and related lib on your device.</p><p>But if you have used HomeBrew to install python, and having problem with running &quot;pip install fedml&quot; command, in this case you need to ensure:</p><ul><li><p>a. Two environment path on your device need to be specify to use Conda not HomeBrew:</p><ul><li>(1) In <code>~/.bash_profile</code> and <code>~/.zprofile</code>, the path to python bin file, need to be Conda python file location.</li><li>(2) In <code>~/.bash_profile</code> and <code>~/.zprofile</code>, the path to pip bin file, need to be Conda pip file location.</li></ul></li><li><p>b. When you encounter with C/C++ compiler issue, try: <code>conda install …</code></p></li></ul><h2 id="installing-fedml-from-debugging-and-editable-mode" tabindex="-1"><a class="header-anchor" href="#installing-fedml-from-debugging-and-editable-mode" aria-hidden="true">#</a> Installing FedML from Debugging and Editable Mode</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> python
pip <span class="token function">install</span> <span class="token parameter variable">-e</span> ./
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="installing-fedml-from-source" tabindex="-1"><a class="header-anchor" href="#installing-fedml-from-source" aria-hidden="true">#</a> Installing FedML from Source</h2><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">git</span> clone https://github.com/FedML-AI/FedML.git <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\\</span>
<span class="token builtin class-name">cd</span> ./FedML/python <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\\</span>
python setup.py <span class="token function">install</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>If you want to run examples with TensorFlow, Jax or MxNet, you need to install optional dependencies:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">git</span> clone https://github.com/FedML-AI/FedML.git <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\\</span>
<span class="token builtin class-name">cd</span> ./FedML/python <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\\</span>
python <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token string">&#39;.[tensorflow]&#39;</span>
python <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token string">&#39;.[jax]&#39;</span>
python <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token string">&#39;.[mxnet]&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>(<strong>Notes</strong>: Tensorflow example located in tf_mqtt_s3_fedavg_mnist_lr_example directory, Jax example location in jax_haiku_mqtt_s3_fedavg_mnist_lr_example directory)</p><p>If you need to install from a specific commit (normally used for the debugging/development phase), please follow commands below:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">git</span> clone https://github.com/FedML-AI/FedML.git <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\\</span>
<span class="token builtin class-name">cd</span> ./FedML <span class="token operator">&amp;&amp;</span> <span class="token function">git</span> checkout e798061d62560b03e049d514e7cc8f1a753fde6b <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\\</span>
<span class="token builtin class-name">cd</span> python <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\\</span>
python setup.py <span class="token function">install</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,22),_={href:"https://github.com/FedML-AI/FedML/commits/master",target:"_blank",rel:"noopener noreferrer"},w=a("hr",null,null,-1),M=a("h2",{id:"running-fedml-in-docker-recommended",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#running-fedml-in-docker-recommended","aria-hidden":"true"},"#"),e(" Running FedML in Docker (Recommended)")],-1),L={href:"https://hub.docker.com/repository/docker/fedml/fedml",target:"_blank",rel:"noopener noreferrer"},O={href:"https://github.com/FedML-AI/FedML/tree/master/installation/build_fedml_docker",target:"_blank",rel:"noopener noreferrer"},C=t(`<p>Please refer to the following commands and remember to change <code>LOCAL_WORKSPACE</code> to your own.</p><h3 id="fedml-standard-docker-image" tabindex="-1"><a class="header-anchor" href="#fedml-standard-docker-image" aria-hidden="true">#</a> FedML Standard Docker Image</h3><p>The FedML standard docker image can support to run on CPU an GPU devices. It deviated from the Nvidia official image which is large size. So the FedML standard docker image will be a large image. Now it is about 17GB in size. Up to now, the FedML standard docker image can run on the Linux platform. If you want to run on the MacOS platform, you should use the FedML light docker image which can be running on multiple architectures, e.g. X86, ARM, etc.</p><p><strong>(1) Pull the standard Docker image and prepare the docker environment</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token assign-left variable">FEDML_DOCKER_IMAGE</span><span class="token operator">=</span>fedml/fedml:latest-torch1.13.1-cuda11.6-cudnn8-devel
<span class="token function">docker</span> pull <span class="token variable">$FEDML_DOCKER_IMAGE</span>

<span class="token comment"># if you want to use GPUs in your host OS, please follow this link: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker</span>
<span class="token function">sudo</span> <span class="token function">apt-get</span> update
<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> nvidia-docker2
<span class="token function">sudo</span> systemctl restart <span class="token function">docker</span>
<span class="token function">sudo</span> <span class="token function">chmod</span> <span class="token number">777</span> /var/run/docker.sock
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>(2) Run standard Docker with interactive mode</strong></p><p><em><strong>On GPUs:</strong></em></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token assign-left variable">FEDML_DOCKER_IMAGE</span><span class="token operator">=</span>fedml/fedml:latest-torch1.13.1-cuda11.6-cudnn8-devel
<span class="token assign-left variable">LOCAL_WORKSPACE</span><span class="token operator">=</span><span class="token variable">$PleaseUseYourLocalDirectory</span>
<span class="token assign-left variable">DOCKER_WORKSPACE</span><span class="token operator">=</span>/home/fedml/fedml_source

<span class="token function">docker</span> run <span class="token parameter variable">-v</span> <span class="token variable">$LOCAL_WORKSPACE</span><span class="token builtin class-name">:</span><span class="token variable">$DOCKER_WORKSPACE</span> --shm-size<span class="token operator">=</span>64g <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">nofile</span><span class="token operator">=</span><span class="token number">65535</span> <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">memlock</span><span class="token operator">=</span>-1 <span class="token parameter variable">--privileged</span> <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">--network</span><span class="token operator">=</span>host <span class="token parameter variable">--env</span> <span class="token assign-left variable">WORKSPACE</span><span class="token operator">=</span><span class="token variable">$DOCKER_WORKSPACE</span> <span class="token parameter variable">-ti</span> <span class="token variable">$FEDML_DOCKER_IMAGE</span> /bin/bash
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><em><strong>On CPUs:</strong></em></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token assign-left variable">FEDML_DOCKER_IMAGE</span><span class="token operator">=</span>fedml/fedml:latest-torch1.13.1-cuda11.6-cudnn8-devel
<span class="token assign-left variable">LOCAL_WORKSPACE</span><span class="token operator">=</span><span class="token variable">$PleaseUseYourLocalDirectory</span>
<span class="token assign-left variable">DOCKER_WORKSPACE</span><span class="token operator">=</span>/home/fedml/fedml_source

ddocker run <span class="token parameter variable">-v</span> <span class="token variable">$LOCAL_WORKSPACE</span><span class="token builtin class-name">:</span><span class="token variable">$DOCKER_WORKSPACE</span> --shm-size<span class="token operator">=</span>64g <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">nofile</span><span class="token operator">=</span><span class="token number">65535</span> <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">memlock</span><span class="token operator">=</span>-1 <span class="token parameter variable">--privileged</span> <span class="token parameter variable">--network</span><span class="token operator">=</span>host <span class="token parameter variable">--env</span> <span class="token assign-left variable">WORKSPACE</span><span class="token operator">=</span><span class="token variable">$DOCKER_WORKSPACE</span> <span class="token parameter variable">-ti</span> <span class="token variable">$FEDML_DOCKER_IMAGE</span> /bin/bash
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You should now see a prompt that looks something like, you may run the &#39;fedml login $YourUserId&#39; to log into the MLOps platform.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>root@142ffce4cdf8:/<span class="token comment">#</span>
root@142ffce4cdf8:/<span class="token comment"># fedml login 1606</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>And also, you may enter into the $WORKSPACE which is your host directory to run your own examples:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>root@142ffce4cdf8:/<span class="token comment">#</span>
root@142ffce4cdf8:/<span class="token comment"># cd $WORKSPACE</span>
root@142ffce4cdf8:/home/fedml/fedml_source<span class="token comment">#</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="fedml-light-docker-image" tabindex="-1"><a class="header-anchor" href="#fedml-light-docker-image" aria-hidden="true">#</a> FedML Light Docker Image</h3><p><strong>(1) Run light Docker with interactive mode</strong></p><p>The light docker is a smaller image about 2.3GB size. So it can pull and run more smoothly. The light docker just supports cpu arch. So, if you want to use the GPU, you should use the above standard Docker with gpu options. Each docker image needs more than 5GB memory size to run the fedml learning task. (<em>This is estimated with the MNist dataset, if you use other dataset, Maybe the memory size is larger or smaller than the size with the MNist dataset</em>) So, you need to reserve sufficient memory size for your federated learning task. On MacOS, you should set memory size in the navigation path <code>DockerDesktop -&gt; Preference -&gt; Resource -&gt; Memory</code>. If you want to run three docker containers simultaneously, you need to set the resource memory to not less than 15GB.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token assign-left variable">FEDML_DOCKER_IMAGE</span><span class="token operator">=</span>fedml/fedml:light
<span class="token assign-left variable">LOCAL_WORKSPACE</span><span class="token operator">=</span><span class="token variable">$PleaseUseYourLocalDirectory</span>
<span class="token assign-left variable">DOCKER_WORKSPACE</span><span class="token operator">=</span>/home/fedml/fedml_source

<span class="token function">docker</span> run <span class="token parameter variable">-v</span> <span class="token variable">$LOCAL_WORKSPACE</span><span class="token builtin class-name">:</span><span class="token variable">$DOCKER_WORKSPACE</span> --shm-size<span class="token operator">=</span>64g <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">nofile</span><span class="token operator">=</span><span class="token number">65535</span> <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">memlock</span><span class="token operator">=</span>-1 <span class="token parameter variable">--privileged</span> <span class="token parameter variable">--network</span><span class="token operator">=</span>host <span class="token parameter variable">--env</span> <span class="token assign-left variable">WORKSPACE</span><span class="token operator">=</span><span class="token variable">$DOCKER_WORKSPACE</span> <span class="token parameter variable">-ti</span> <span class="token variable">$FEDML_DOCKER_IMAGE</span> /bin/bash
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You should now see a prompt that looks something like, you may run the &#39;fedml login $YourUserId&#39; to log into the MLOps platform.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>root@142ffce4cdf8:/<span class="token comment">#</span>
root@142ffce4cdf8:/<span class="token comment"># fedml login 1606</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>And also, you may enter into the $WORKSPACE which is your host directory to run your own examples:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>root@142ffce4cdf8:/<span class="token comment">#</span>
root@142ffce4cdf8:/<span class="token comment"># cd $WORKSPACE</span>
root@142ffce4cdf8:/home/fedml/fedml_source<span class="token comment">#</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>(2) Run light Docker with daemon mode and automatically log into the MLOps platform</strong></p><p>You may run the light docker as the daemon mode and automatically log into the MLOps platform as the client. The commands ars as follows:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token assign-left variable">FEDML_DOCKER_IMAGE</span><span class="token operator">=</span>fedml/fedml:light
<span class="token assign-left variable">LOCAL_WORKSPACE</span><span class="token operator">=</span><span class="token variable">$PleaseUseYourLocalDirectory</span>
<span class="token assign-left variable">DOCKER_WORKSPACE</span><span class="token operator">=</span>/home/fedml/fedml_source
<span class="token assign-left variable">YOUR_FEDML_USER_ID</span><span class="token operator">=</span><span class="token number">1606</span>

<span class="token function">docker</span> run <span class="token parameter variable">-v</span> <span class="token variable">$LOCAL_WORKSPACE</span><span class="token builtin class-name">:</span><span class="token variable">$DOCKER_WORKSPACE</span> --shm-size<span class="token operator">=</span>64g <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">nofile</span><span class="token operator">=</span><span class="token number">65535</span> <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">memlock</span><span class="token operator">=</span>-1 <span class="token parameter variable">--privileged</span> <span class="token parameter variable">--network</span><span class="token operator">=</span>host <span class="token parameter variable">--env</span> <span class="token assign-left variable">WORKSPACE</span><span class="token operator">=</span><span class="token variable">$DOCKER_WORKSPACE</span> <span class="token parameter variable">-d</span> <span class="token variable">$FEDML_DOCKER_IMAGE</span> <span class="token function">bash</span> <span class="token parameter variable">-c</span> <span class="token string">&#39;fedml login &#39;</span><span class="token variable">$YOUR_FEDML_USER_ID</span><span class="token string">&#39;;sleep 100000&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You may run the light docker as the daemon mode and automatically log into the MLOps platform as the server with the option <code>-s</code>. The commands ars as follows:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token assign-left variable">FEDML_DOCKER_IMAGE</span><span class="token operator">=</span>fedml/fedml:light
<span class="token assign-left variable">LOCAL_WORKSPACE</span><span class="token operator">=</span><span class="token variable">$PleaseUseYourLocalDirectory</span>
<span class="token assign-left variable">DOCKER_WORKSPACE</span><span class="token operator">=</span>/home/fedml/fedml_source
<span class="token assign-left variable">YOUR_FEDML_USER_ID</span><span class="token operator">=</span><span class="token number">1606</span>

<span class="token function">docker</span> run <span class="token parameter variable">-v</span> <span class="token variable">$LOCAL_WORKSPACE</span><span class="token builtin class-name">:</span><span class="token variable">$DOCKER_WORKSPACE</span> --shm-size<span class="token operator">=</span>64g <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">nofile</span><span class="token operator">=</span><span class="token number">65535</span> <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">memlock</span><span class="token operator">=</span>-1 <span class="token parameter variable">--privileged</span> <span class="token parameter variable">--network</span><span class="token operator">=</span>host <span class="token parameter variable">--env</span> <span class="token assign-left variable">WORKSPACE</span><span class="token operator">=</span><span class="token variable">$DOCKER_WORKSPACE</span> <span class="token parameter variable">-d</span> <span class="token variable">$FEDML_DOCKER_IMAGE</span> <span class="token function">bash</span> <span class="token parameter variable">-c</span> <span class="token string">&#39;fedml login -s &#39;</span><span class="token variable">$YOUR_FEDML_USER_ID</span><span class="token string">&#39;;sleep 100000&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>After you run the above command, the terminal will show the container id like the following format. <code>b0769135f8e65c5b0b7b7cb9666f3f910a4e431c25084ed72ae059ea1a6376af</code></p><p>If you want to show logs for the fedml light container, you may run the following command with the above container id.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> logs b0769135f8e65c5b0b7b7cb9666f3f910a4e431c25084ed72ae059ea1a6376af
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>If you want to list the fedml light containers, you may run the command.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> <span class="token function">ps</span> <span class="token operator">|</span><span class="token function">grep</span> fedml:light
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>If you want to kill all fedml light containers, the command is as follows.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">docker</span> stop <span class="token variable"><span class="token variable">\`</span><span class="token function">docker</span> <span class="token function">ps</span> <span class="token operator">|</span><span class="token function">grep</span> fedml:light <span class="token operator">|</span><span class="token function">awk</span> -F<span class="token string">&#39; &#39;</span> <span class="token string">&#39;{print $1}&#39;</span><span class="token variable">\`</span></span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p><strong>(4) Run the interpreter in PyCharm or Visual Studio using Docker environment</strong></p><ul><li>PyCharm</li></ul>`,36),E={href:"https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html#summary",target:"_blank",rel:"noopener noreferrer"},A=a("ul",null,[a("li",null,"Visual Studio")],-1),x={href:"https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html#summary",target:"_blank",rel:"noopener noreferrer"},R=t(`<p><strong>(4) Other useful commands</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># docker rm $(docker ps -aq)</span>
<span class="token function">docker</span> container <span class="token function">kill</span> <span class="token variable"><span class="token variable">$(</span><span class="token function">docker</span> <span class="token function">ps</span> <span class="token parameter variable">-q</span><span class="token variable">)</span></span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="running-fedml-on-kubernetes" tabindex="-1"><a class="header-anchor" href="#running-fedml-on-kubernetes" aria-hidden="true">#</a> Running FedML on Kubernetes</h2><p>This tutorial will guide you to deploy your fedml client and server to target Kubernetes pods running on GPU/CPU physical nodes.</p><p>The entire workflow is as follows:</p>`,5),I={href:"https://github.com/FedML-AI/FedML/tree/master/python/examples",target:"_blank",rel:"noopener noreferrer"},D=a("li",null,"In the file fedml-edge-client-server/deployment-client.yml, modify the variable ACCOUNT_ID to your desired value",-1),P=a("li",null,[e("Deploy the fedml client: "),a("code",null,"kubectl apply -f ./fedml-edge-client-server/deployment-client.yml")],-1),F=a("li",null,"In the file fedml-edge-client-server/deployment-server.yml, modify the variable ACCOUNT_ID to your desired value",-1),S=a("li",null,[e("Deploy the fedml server: "),a("code",null,"kubectl apply -f ./fedml-edge-client-server/deployment-server.yml")],-1),K={href:"https://open.fedml.ai",target:"_blank",rel:"noopener noreferrer"},$=t(`<p>If you want to scale up or scal down the pods to your desired count, you may run the following command:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>kubectl scale <span class="token parameter variable">-n</span> <span class="token variable">$YourNameSpace</span> <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token variable">$YourDesiredPodsCount</span> deployment/fedml-client-deployment
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>kubectl scale <span class="token parameter variable">-n</span> <span class="token variable">$YourNameSpace</span> <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token variable">$YourDesiredPodsCount</span> deployment/fedml-server-deployment
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="installation-with-helm-charts" tabindex="-1"><a class="header-anchor" href="#installation-with-helm-charts" aria-hidden="true">#</a> Installation with Helm Charts</h2><p>Also, you may use the helm charts to deploy your fedml client and server to target Kubernetes cluster. You just need to run the following commands with your user id at the open.fedml.ai.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>kubectl create namespace fedml
helm <span class="token function">install</span> <span class="token parameter variable">--set</span> <span class="token assign-left variable">image.repository</span><span class="token operator">=</span><span class="token string">&quot;fedml/fedml-edge-client-server-light&quot;</span> <span class="token parameter variable">--set</span> <span class="token assign-left variable">env.fedmlAccountId</span><span class="token operator">=</span><span class="token string">&quot;<span class="token variable">$YourUserId</span>&quot;</span> <span class="token parameter variable">--set</span> <span class="token assign-left variable">env.role</span><span class="token operator">=</span><span class="token string">&quot;client&quot;</span> fedml-client-deployment ./fedml-client-deployment-latest.tgz
helm <span class="token function">install</span> <span class="token parameter variable">--set</span> <span class="token assign-left variable">image.repository</span><span class="token operator">=</span><span class="token string">&quot;fedml/fedml-edge-client-server-light&quot;</span> <span class="token parameter variable">--set</span> <span class="token assign-left variable">env.fedmlAccountId</span><span class="token operator">=</span><span class="token string">&quot;<span class="token variable">$YourUserId</span>&quot;</span> <span class="token parameter variable">--set</span> <span class="token assign-left variable">env.role</span><span class="token operator">=</span><span class="token string">&quot;server&quot;</span> fedml-server-deployment ./fedml-server-deployment-latest.tgz
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h1 id="q-a" tabindex="-1"><a class="header-anchor" href="#q-a" aria-hidden="true">#</a> Q&amp;A</h1><ol><li>Q: How to scale up or scale down?<br> A: Use the following commands:</li></ol><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>kubectl scale <span class="token parameter variable">-n</span> <span class="token variable">$YourNameSpace</span> <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token variable">$YourDesiredPodsCount</span> deployment/fedml-client-deployment
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>kubectl scale <span class="token parameter variable">-n</span> <span class="token variable">$YourNameSpace</span> <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token variable">$YourDesiredPodsCount</span> deployment/fedml-server-deployment
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ol start="2"><li><p>Q: FedML Client send online status to FedML Server via which protocol?<br> A: Via MQTT</p></li><li><p>Q: FedML Client send model, gradient parameters to FedML Server via which protocol?<br> A: Use S3 protocol to store and exchange models and use MQTT to exchange messages between FedML Client and Server</p></li><li><p>Q: Why do we need AWS S3?<br> A: Use S3 protocol to store and exchange models.</p></li></ol><h2 id="guidance-for-windows-users" tabindex="-1"><a class="header-anchor" href="#guidance-for-windows-users" aria-hidden="true">#</a> Guidance for Windows Users</h2>`,12),W=a("h2",{id:"guidance-for-raspberry-pi-users",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#guidance-for-raspberry-pi-users","aria-hidden":"true"},"#"),e(" Guidance for Raspberry Pi Users")],-1),q=a("h2",{id:"guidance-for-nvidia-jetson-devices",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#guidance-for-nvidia-jetson-devices","aria-hidden":"true"},"#"),e(" Guidance for NVIDIA Jetson Devices")],-1),U=t(`<h2 id="testing-if-the-installation-succeeded" tabindex="-1"><a class="header-anchor" href="#testing-if-the-installation-succeeded" aria-hidden="true">#</a> Testing if the installation succeeded</h2><p>If the installation is successful, you will not see any issue when run <code>import fedml</code>.</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token punctuation">(</span>mnn37<span class="token punctuation">)</span> chaoyanghe@Chaoyangs-MBP FedML-refactor % python
Python <span class="token number">3.7</span>.7 <span class="token punctuation">(</span>default, Mar <span class="token number">26</span> <span class="token number">2020</span>, <span class="token number">10</span>:32:53<span class="token punctuation">)</span> 
<span class="token punctuation">[</span>Clang <span class="token number">4.0</span>.1 <span class="token punctuation">(</span>tags/RELEASE_401/final<span class="token punctuation">)</span><span class="token punctuation">]</span> :: Anaconda, Inc. on darwin
Type <span class="token string">&quot;help&quot;</span>, <span class="token string">&quot;copyright&quot;</span>, <span class="token string">&quot;credits&quot;</span> or <span class="token string">&quot;license&quot;</span> <span class="token keyword">for</span> <span class="token function">more</span> information.
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token function">import</span> fedml
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> 

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="installing-fedml-android-sdk-app" tabindex="-1"><a class="header-anchor" href="#installing-fedml-android-sdk-app" aria-hidden="true">#</a> Installing FedML Android SDK/APP</h2><p>Please follow the instructions at <code>https://github.com/FedML-AI/FedML/java/README.md</code></p><h2 id="troubleshooting" tabindex="-1"><a class="header-anchor" href="#troubleshooting" aria-hidden="true">#</a> Troubleshooting</h2>`,6),T={href:"https://github.com/FedML-AI/FedML/issues",target:"_blank",rel:"noopener noreferrer"};function Y(G,N){const n=i("ExternalLinkIcon"),l=i("RouterLink");return p(),c("div",null,[u,m,h,a("p",null,[a("a",v,[e("https://github.com/FedML-AI/FedML"),s(n)])]),b,a("p",null,[e("About OpenMPI library installation for MPI, the reference is as follows: "),a("a",k,[e("https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html"),s(n)]),e(", For OpenMPI on MacOS, please review the following links: "),a("a",g,[e("https://betterprogramming.pub/integrating-open-mpi-with-clion-on-apple-m1-76b7815c27f2"),s(n)]),e(" and "),a("a",f,[e("https://formulae.brew.sh/formula/open-mpi"),s(n)])]),y,a("p",null,[e("Please change the above commit id to your own (you can find it at "),a("a",_,[e("https://github.com/FedML-AI/FedML/commits/master"),s(n)]),e(")")]),w,M,a("p",null,[e("FedML Docker Hub: "),a("a",L,[e("https://hub.docker.com/repository/docker/fedml/fedml"),s(n)])]),a("p",null,[e("We recommend using FedML in the Docker environment as it circumvents complex and tedious installation debugging. Currently, we maintain docker images for x86_64 architecture. But for your own purpose, you may build your docker image to support the following architectures: arm, raspberrypi, nvidia jetson via our Dockerfile located in the directory "),a("a",O,[e("https://github.com/FedML-AI/FedML/tree/master/installation/build_fedml_docker"),s(n)])]),C,a("p",null,[a("a",E,[e("https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html#summary"),s(n)])]),A,a("p",null,[a("a",x,[e("https://code.visualstudio.com/docs/remote/containers"),s(n)])]),R,a("p",null,[e("(k8s deployment file is located in: "),a("a",I,[e("https://github.com/FedML-AI/FedML/tree/master/installation/install_on_k8s/fedml-edge-client-server"),s(n)]),e(")")]),a("ol",null,[D,P,F,S,a("li",null,[e("Login the FedML MLOps platform "),a("a",K,[e("https://open.fedml.ai"),s(n)]),e(", the above deployed client and server will be found in the edge devices")])]),$,a("p",null,[e("Please follow instructions at "),s(l,{to:"/starter/install/windows.html"},{default:o(()=>[e("Windows Installation")]),_:1})]),W,a("p",null,[e("Please follow instructions at "),s(l,{to:"/starter/install/rpi.html"},{default:o(()=>[e("Raspberry Pi Installation")]),_:1})]),q,a("p",null,[e("Please follow instructions at "),s(l,{to:"/starter/install/jetson.html"},{default:o(()=>[e("NVIDIA Jetson Device Installation")]),_:1})]),U,a("p",null,[e("If you met any issues during installation or have additional installation requirements, please post issues at "),a("a",T,[e("https://github.com/FedML-AI/FedML/issues"),s(n)])])])}const j=r(d,[["render",Y],["__file","installation.html.vue"]]);export{j as default};
